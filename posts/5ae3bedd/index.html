<!doctype html><html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta><title>ONNX算子简介 - CuiYuhao&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="CuiYuhao&#039;s Blog"><meta name="msapplication-TileImage" content="/img/webicon.jpg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="CuiYuhao&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="详细英文介绍见ONNX算子 （算子较多，善用Ctrl+F查找）"><meta property="og:type" content="blog"><meta property="og:title" content="ONNX算子简介"><meta property="og:url" content="https://cyhasuka.github.io/posts/5ae3bedd/"><meta property="og:site_name" content="CuiYuhao&#039;s Blog"><meta property="og:description" content="详细英文介绍见ONNX算子 （算子较多，善用Ctrl+F查找）"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://cyhasuka.github.io/img/og_image.png"><meta property="article:published_time" content="2024-04-01T09:04:05.000Z"><meta property="article:modified_time" content="2024-04-01T10:08:26.371Z"><meta property="article:author" content="cyhasuka"><meta property="article:tag" content="Linux"><meta property="article:tag" content="感知算法"><meta property="article:tag" content="深度学习"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://cyhasuka.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://cyhasuka.github.io/posts/5ae3bedd/"},"headline":"ONNX算子简介","image":["https://cyhasuka.github.io/img/og_image.png"],"datePublished":"2024-04-01T09:04:05.000Z","dateModified":"2024-04-01T10:08:26.371Z","author":{"@type":"Person","name":"cyhasuka"},"publisher":{"@type":"Organization","name":"CuiYuhao's Blog","logo":{"@type":"ImageObject","url":"https://cyhasuka.github.io/img/toplogo.jpg"}},"description":"详细英文介绍见ONNX算子 （算子较多，善用Ctrl+F查找）"}</script><link rel="canonical" href="https://cyhasuka.github.io/posts/5ae3bedd/"><link rel="icon" href="/img/webicon.jpg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><script>!function(){function e(){if(!location.hash)return;const e="#"+CSS.escape(location.hash.substring(1)),t=document.querySelector(`.tabs a[href="${e}"]`);if(!t)return;const n=t.parentElement.parentElement;Array.from(n.children).forEach(e=>e.classList.remove("is-active")),Array.from(n.querySelectorAll("a")).map(e=>document.getElementById(e.getAttribute("href").substring(1))).forEach(e=>e.classList.add("is-hidden")),t&&t.parentElement.classList.add("is-active");const r=document.querySelector(e);r&&r.classList.remove("is-hidden")}e(),window.addEventListener("hashchange",e,!1)}()</script><meta name="generator" content="Hexo 7.0.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/toplogo.jpg" alt="CuiYuhao&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/index">首页</a><a class="navbar-item" href="/archives">文章</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/posts/c0297c2d">留言板</a><a class="navbar-item" href="/about">个人简介</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="我的GitHub" href="https://github.com/cyhasuka"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2024-04-01T09:04:05.000Z" title="4/1/2024, 5:04:05 PM">2024-04-01</time>发表</span><span class="level-item"><time datetime="2024-04-01T10:08:26.371Z" title="4/1/2024, 6:08:26 PM">2024-04-01</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%9D%A5%E7%82%B9%E5%B9%B2%E8%B4%A7/">来点干货</a></span><span class="level-item">完整阅读约1 小时 (7666个字)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">ONNX算子简介</h1><div class="content"><p>详细英文介绍见<a target="_blank" rel="noopener" href="https://github.com/onnx/onnx/blob/main/docs/Operators.md">ONNX算子</a></p><p>（算子较多，善用Ctrl+F查找）<br><span id="more"></span></p><div class="table-container"><table><thead><tr><th><strong>序号</strong></th><th><strong>算子</strong></th><th><strong>含义</strong></th><th><strong>参数</strong></th></tr></thead><tbody><tr><td>1</td><td>Abs</td><td>求绝对值</td><td>【输入】<strong>input：输入Tensor，float32</strong><br>【约束】<strong>无限制</strong><br>【输出】**output：输出Tensor</td></tr><tr><td>2</td><td>Acos</td><td>反余弦</td><td>【输入】<strong>input：输入Tensor，数值范围[-1, 1]，类型：float32</strong><br>【约束】<strong>无限制</strong><br>【输出】**output：输出Tensor，数值范围[0, pi]，类型与x输入相同</td></tr><tr><td>3</td><td>Add</td><td>二元点加</td><td><strong>【输入】</strong><br><br>- A：输入Tensor，类型：float32，第1个操作数<br>- B：输入Tensor，类型：float32，第2个操作数<br><br>【约束】<strong>支持两组输入的维度不一致，进行广播操作（广播即维度补齐），目前支持以下几种广播场景：<br><br>- NCHW+NCHW(备注, 两个维度相同tensor)<br>- NCHW+scalar<br>- NCHW+W, CHW+W, HW+W(备注, W维度做broadcast)<br>- NCHW + NCH1, CHW + CH1, HW + H1<br>- CHW + C1W(备注，H维度做broadcast)<br><br>对于两个输入维度个数不相同的场景，需要将维度补齐到四维。例如，x.shape=(1, 5, 6, 7) 和 y.shape=(6, 7)需要将y的维度补齐到4维，即y.shape=(1, 1, 6, 7)。说明：两个Tensor的输入顺序可以互换。<br></strong>【输出】C：输出Tensor，类型同B</td></tr><tr><td>4</td><td>And</td><td>取与运算</td><td><strong>【输入】</strong><br><br>- A：输入Tensor，类型：bool<br>- B：输入Tensor，类型：bool<br><br><strong>【约束】</strong>input输入不支持NCHW格式数据<strong><br>【输出】</strong>output：输出Tensor，类型：bool</td></tr><tr><td>5</td><td>ArgMax</td><td>在指定轴上找到最大值索引</td><td>【输入】<strong>data：输入Tensor，类型：float32</strong><br>【参数】<strong><br>- axis：int，default值为0，取值范围[-r, r-1]，r = rank(data)<br>- keepdims：int，default值为1<br><br></strong>【输出】**output：输出Tensor，类型：int64</td></tr><tr><td>6</td><td>Asin</td><td>反正弦</td><td><strong>【输入】</strong>input：输入Tensor，数值范围[-1, 1]，类型：float32<strong><br><br>【约束】</strong>无限制<strong><br><br>【输出】</strong>output：输出Tensor，数值范围[-pi/2, pi/2]，类型与input输入相同</td></tr><tr><td>7</td><td>Atan</td><td>反正切</td><td><strong>【输入】</strong>input：输入Tensor，类型：float32<strong><br><br>【约束】</strong>无限制<strong><br><br>【输出】</strong>output：输出Tensor，类型与input输入相同</td></tr><tr><td>8</td><td>AveragePool</td><td>平均池化</td><td><strong>【输入】</strong>X：输入Tensor，类型：float32<strong>【参数】</strong><br><br>- auto_pad：optional, string，pad的计算模式，默认是NOTSET<br>- ceil_mode：optional, int，使用ceil（向上取整）还是floor（向下取整）计算输出维度，默认是0（ceil）<br>- count_include_pad：optional，int，计算边缘时是否包含pad，默认是0，不包含pad<br>- kernel_shape：list of ints，每个值对应相应维度的窗口大小<br>- pads：list of ints，每个值对应相应维度的pad值，默认值为0<br>- strides：list of ints，其中每个值对应相应维度的滑动步长，默认值为1<br><br><strong>【约束】</strong><br><br>- auto_pad参数不支持SAME_UPPER，SAME_LOWER<br>- count_include_pad参数只支持默认值<br><br><strong>【输出】</strong>Y：输出Tensor，类型与X输入相同</td></tr><tr><td>9</td><td>BatchNormalization</td><td>对输入做标准化</td><td><strong>【输入】</strong><br><br>- X：输入Tensor，类型：float32<br>- scale：输入Tensor，类型：float32，用于缩放<br>- B：输入Tensor，类型：float32，偏差<br>- mean：输入Tensor，类型：float32，用于推理总体均值<br>- var：输入Tensor，类型：float32，用于推理总体方差<br>- training_mode：输入Tensor，optional，训练模式<br><br><strong>【参数】</strong><br><br>- epsilon：float32，在X的方差中添加的一个小的浮点数，默认值是1e-05<br>- momentum：float32，计算mean和var的因子，默认值为0.9<br>- spatial：int，计算mean和var的方式，默认值1<br><br><strong>【约束】</strong><br><br>- 不支持训练场景<br>- 不支持training_mode输入<br>- 不支持output_mean、output_var、saved_mean、saved_var输出<br>- opset7中不支持spatial设置<br><br><strong>【输出】</strong><br><br>- Y：输出Tensor，和X输入有相同的维度<br>- output_mean：训练模式下是滑动均值，非训练模式下是估计均值<br>- output_var：训练模式下是滑动方差，非训练模式下是估计方差<br>- saved_mean：已保存的均值<br>- saved_var：已保存的均值</td></tr><tr><td>10</td><td>Cast</td><td>数据类型转换</td><td><strong>【输入】</strong>input：输入Tensor，类型：float32，bool，int32，uint8<strong>【参数】</strong>to：数据类型int<br><br>- FLOAT = 1<br>- UINT8 = 2<br>- INT8 = 3<br>- UINT16 = 4<br>- INT16 = 5<br>- INT32 = 6<br>- INT64 = 7<br>- STRING = 8<br>- BOOL = 9<br>- FLOAT16 = 10<br>- DOUBLE = 11<br>- UINT32 = 12<br><br><strong>【约束】</strong>支持下面类型转换：<br><br>- fp16 -&gt; fp32<br>- fp32 -&gt; fp16<br>- u8 -&gt; fp16<br>- fp16 -&gt; u8<br>- int32 -&gt; fp32<br>- fp32-&gt;int32<br>- fp16 -&gt; int8<br>- int8 -&gt; fp16<br>- in32 -&gt; fp16<br>- fp16 -&gt; int32<br>- bool -&gt; fp16<br>- fp16 -&gt; bool<br><br><strong>【输出】</strong>output：输出Tensor</td></tr><tr><td>11</td><td>Ceil</td><td>向上取值</td><td><strong>【输入】</strong>X：输入Tensor，类型：float32<strong><br><br>【约束】</strong>无<strong><br><br>【输出】</strong>output：输出Tensor，类型与input输入相同</td></tr><tr><td>12</td><td>Clip</td><td>将输入限制在一个区间中</td><td><strong>【输入】</strong><br><br>- input：输入Tensor，类型：float32<br>- min：Scalar Tensor，optional，类型：float32，区间最小值<br>- max：Scalar Tensor，optional，类型：float32，区间最大值<br><br><strong>【参数】</strong>在opset 7~10中min和max是参数<strong>【约束】</strong><br><br>- min输入必须是常量<br>- max输入必须是常量<br><br><strong>【输出】</strong>output：输出Tensor，类型与input输入相同</td></tr><tr><td>13</td><td>Concat</td><td>数据按维度拼接</td><td><strong>【输入】</strong><br><br>- inputs：List of tensors，类型：float32，int32<br><br><strong>【参数】</strong><br><br>- axis：int，轴参数，控制需要拼接的数据轴，负值表示从维度最后一位往前数<br><br><strong>【约束】</strong>无<strong><br><br>【输出】</strong>concat_result：拼接之后的Tensor，类型：float32，int32</td></tr><tr><td>14</td><td>Constant</td><td>输出1个常量Tensor</td><td><strong>【输入】</strong>无<strong>【参数】</strong><br><br>- sparse_value：稀疏类型的输出值<br>- value：Tensor，输出Tensor的值<br>- value_float：float32类型的标量输出Tensor的值<br>- value_floats：list of floats，float32类型的1D输出Tensor的值<br>- value_int：int，int32类型的标量输出Tensor的值<br>- value_ints：list of ints，int32类型的1D输出Tensor的值<br>- value_string：string，string类型的标量UTF-8输出Tensor的值<br>- value_strings：list of strings，string类型的1D UTF-8输出Tensor的值<br><br><strong>【约束】</strong><br><br>- 不支持sparse_value参数<br>- 不支持value_string参数<br>- 不支持value_strings参数<br><br><strong>【输出】</strong>output：输出Tensor，和提供的Tensor具有相同的值</td></tr><tr><td>15</td><td>ConstantOfShape</td><td>根据给定的值和维度，生成1个Tensor</td><td><strong>【输入】</strong>input：1-D Tensor，类型：int32，uint8<strong><br><br>【参数】</strong>value：optional，0-D Tensor，需要填充的值，默认为0，默认类型float32<strong><br><br>【约束】</strong>无<strong><br><br>【输出】</strong>output：输出Tensor，类型同value</td></tr><tr><td>16</td><td>Conv</td><td>卷积</td><td><strong>【输入】</strong><br><br>- X：输入Tensor，类型：float32<br>- W：输入Tensor，具有维度(M <em>C/group </em>kH <em>kW)，其中C是channels的数量，kH和kW是卷积核的高和宽，M是feature maps的数量<br>- B：optional，1D 常量Tensor，卷积计算时添加的偏置<br><br><strong>【参数】</strong><br><br>- auto_pad：optional, string，pad的计算模式，默认是NOTSET<br>- dilations：optional，list of ints，每个值对应卷积核对应空间轴上的扩张值，默认值1<br>- group：optional，int，组的数量<br>- kernel_shape：list of ints，卷积核的维度<br>- pads：list of ints，每个值对应相应维度的pad值<br>- list of ints，每个值对应相应维度的滑动步长，默认值为1<br><br><strong>【约束】</strong>auto_pad参数不支持SAME_UPPER, SAME_LOWER<em>*【输出】</em></em>Y：输出Tensor，类型同X</td></tr><tr><td>17</td><td>ConvTranspose</td><td>反卷积</td><td><strong>【输入】</strong><br><br>- X：输入Tensor，类型：float32<br>- W：输入Tensor，具有维度(M <em>C/group </em>kH <em>kW)，其中C是channels的数量，kH和kW是卷积核的高和宽，M是feature maps的数量<br>- B：optional，1D 常量Tensor，卷积计算时添加的偏置<br><br><strong>【参数】</strong><br><br>- auto_pad：optional, string，pad的计算模式，默认是NOTSET<br>- dilations：optional，list of ints，每个值对应卷积核对应空间轴上的扩张值，默认值1<br>- group：optional，int，组的数量<br>- kernel_shape：list of ints，卷积核的维度<br>- output_padding：为输出坐标指数较高的边添加的额外值<br>- output_shape：输出的shape<br>- pads：list of ints，每个值对应相应维度的pad值<br>- list of ints，每个值对应相应维度的滑动步长，默认值为1<br><br><strong>【约束】</strong><br><br>- auto_pad参数不支持SAME_UPPER, SAME_LOWER<br>- output_padding参数不支持<br>- W输入必须为常量<br><br><em>*【输出】</em></em>Y：输出Tensor，类型同X输入</td></tr><tr><td>18</td><td>Cos</td><td>计算余弦</td><td><strong>【输入】</strong>input：输入Tensor，类型：float32<strong>【约束】</strong>无限制<strong>【输出】</strong>output：输出Tensor，类型与x输入相同</td></tr><tr><td>19</td><td>DepthToSpace</td><td>重组数据，根据blocksize</td><td><strong>【输入】</strong>input：输入Tensor，类型：float32<strong>【参数】</strong><br><br>- blocksize：数据类型：int<br>- mode: 数据类型 string<br><br><strong>【约束】</strong>mode仅支持DCR CRD两种模式<strong>【输出】</strong>output：输出Tensor，类型与x输入相同</td></tr><tr><td>20</td><td>Div</td><td>做除法运算</td><td><strong>【输入】</strong><br><br>- A：输入Tensor，类型：float32，int32<br>- B：输入Tensor，类型：float32，int32<br><br><strong>【约束】</strong>对于两个输入维度个数不相同的场景，需要将维度补齐到四维。例如，x.shape=(1, 5, 6, 7) 和 y.shape=(6, 7)需要将y的维度补齐到4维，即y.shape=(1, 1, 6, 7)。<br><br><strong>【输出】</strong>C：输出Tensor，类型与x输入相同</td></tr><tr><td>21</td><td>Elu</td><td>根据f(x) = alpha * (exp(x) - 1.) 该公式做计算</td><td><strong>【输入】</strong>X：输入Tensor，类型：float32<strong><br><br>【参数】</strong>alpha：float32，缺省值为1.0<strong><br><br>【约束】</strong>无<strong><br><br>【输出】</strong>Y：输出Tensor，类型与x输入相同</td></tr><tr><td>22</td><td>Equal</td><td>判断输入是否相等</td><td><strong>【输入】</strong><br><br>- A：输入Tensor，类型：uint8,float32,bool,int32（int32只支持标量）<br>- B：输入Tensor，类型：uint8,float32,bool,int32（int32只支持标量）<br><br><strong>【约束】</strong>暂不支持广播场景<strong>【输出】</strong>C：输出Tensor，类型 bool</td></tr><tr><td>23</td><td>Erf</td><td>对输入数据逐个元素做error function计算</td><td><strong>【输入】</strong>x：输入Tensor，类型：float32<strong><br><br>【约束】</strong>无限制<strong><br><br>【输出】</strong>y：输出Tensor，类型与x输入相同</td></tr><tr><td>24</td><td>Exp</td><td>指数函数，output = e^input</td><td><strong>【输入】</strong>input：输入Tensor，类型：float32，double<strong><br><br>【约束】</strong>无限制<strong><br><br>【输出】</strong>output：输出Tensor，类型与x输入相同</td></tr><tr><td>25</td><td>Expand</td><td>根据指定的shape做广播</td><td><strong>【输入】</strong><br><br>- input：输入Tensor，类型：float32，int8, uint8, bool<br>- shape：输入Tensor，类型：int32，1D的tensor，指定输出的shape<br><br><strong>【约束】</strong><br><br>- 支持任意满足broadcast条件的Broadcast场景<br>- 对于需要插broadcastTo算子的Add、Sub、Mul、Div、Max，不支持量化功能<br>- 支持在Kirin 9000及以后的芯片上运行<br><br><strong>【输出】</strong>output：输出Tensor，类型与x输入相同</td></tr><tr><td>26</td><td>Flatten</td><td>数据按维度展开</td><td><strong>【输入】</strong>input：输入 Tensor，类型float32<strong><br><br>【参数】</strong>axis：int，标识数据在哪个维度上展开，值的范围[-r, r]，r是输入Tensor的维度个数，负值表示从最后1个维度往回计算<strong><br><br>【约束】</strong>axis参数必须为1<strong><br><br>【输出】</strong>output：2-D Tensor，类型float32</td></tr><tr><td>27</td><td>Floor</td><td>对输入进行向下取整</td><td><strong>【输入】</strong>X：输入Tensor，类型：float32<strong><br><br>【约束】</strong>无<strong><br><br>【输出】</strong>Y：输出Tensor，类型同X输入</td></tr><tr><td>28</td><td>Gather</td><td>根据输入的indices，从data中获取entry组成输出tensor</td><td><strong>【输入】</strong><br><br>- data：输入Tensor，类型：float32, int32<br>- indices：输入Tensor，数据类型int32<br><br><strong>【参数】</strong>axis：int, [-r, r-1] ,r = rank(data)<strong>【约束】</strong>无<strong>【输出】</strong>output：输出Tensor，类型同data输入</td></tr><tr><td>29</td><td>Gemm</td><td>通用矩阵乘法，Y = alpha <em>A’ </em>B’ + beta * C</td><td><strong>【输入】</strong><br><br>- A：输入Tensor，类型：float32<br>- B：输入Tensor，类型：float32<br>- C：输入Tensor，类型：float32<br><br><strong>【参数】</strong><br><br>- alpha：float32，A <em>B的标量乘数，默认值1.0<br>- beta：float32，C的标量乘数，默认值1.0<br>- transA：int，A输入是否需要转置，默认值0<br>- transB：int，B输入是否需要转置，默认值0<br><br><strong>【约束】</strong><br><br>- transA参数不支持true<br>- 输入B和C只支持常量<br>- 如果A是M</em>K，B是K<em>N，C可以是N或1</em>N或不指定<br><br><strong>【输出】</strong>Y：输出Tensor，维度是（M, N）</td></tr><tr><td>30</td><td>GlobalAveragePool</td><td>对输入进行全局平均池化</td><td><strong>【输入】</strong>X：输入Tensor，类型：float32，维度是（N <em>C </em>H <em>W）<br><br><strong>【约束】</strong>无<em>*<br><br>【输出】</em></em>Y：输出Tensor，类型同X输入</td></tr><tr><td>31</td><td>GlobalMaxPool</td><td>对输入进行全局最大池化</td><td><strong>【输入】</strong>X：输入Tensor，类型float32，维度是（N <em>C </em>H <em>W）<br><br><strong>【约束】</strong>无<em>*<br><br>【输出】</em></em>Y：输出Tensor，类型同X输入</td></tr><tr><td>32</td><td>Greater</td><td>逐个元素比较哪个大</td><td><strong>【输入】</strong><br><br>- A：输入Tensor，类型float32，<br>- B：输入Tensor，类型float32，<br><br><strong>【约束】</strong>无<strong><br><br>【输出】</strong>C：输出Tensor，类型 bool</td></tr><tr><td>33</td><td>InstanceNormalization</td><td>按照下面公式做归一化运算y = scale * (x - mean) / sqrt(variance + epsilon) + B</td><td><strong>【输入】</strong><br><br>- input：输入4D Tensor，类型float32，<br>- scale：输入Tensor，类型float32，<br>- B：输入Tensor，类型float32，<br><br><strong>【参数】</strong>alpha：float32，避免除零错误，默认值1e-05<strong><br><br>【约束】</strong>无<strong><br><br>【输出】</strong>output：输出Tensor，类型同input输入</td></tr><tr><td>34</td><td>LeakyRelu</td><td>对输入进行LeakyRelu激活函数计算f(x) = alpha * x for x &lt; 0, f(x) = x for x &gt;= 0</td><td><strong>【输入】</strong>X：输入Tensor，类型：float32<strong><br><br>【参数】</strong>alpha：float32，泄漏系数，默认值0.01<strong><br><br>【约束】</strong>无<strong><br><br>【输出】</strong>Y：输出Tensor，类型同X输入</td></tr><tr><td>35</td><td>Less</td><td>对输入A和B进行逐元素Less逻辑运算</td><td><strong>【输入】</strong><br><br>- A：输入Tensor，类型：float32<br>- B：输入Tensor，类型：float32<br><br><strong>【约束】</strong>无限制<strong><br><br>【输出】</strong>C：输出Tensor，类型：bool</td></tr><tr><td>36</td><td>Log</td><td>取自然对数运算</td><td><strong>【输入】</strong>input：输入Tensor，类型： float32<strong><br><br>【约束】</strong>无<strong><br><br>【输出】</strong>output：输出Tensor，类型同input输入</td></tr><tr><td>37</td><td>LogSoftmax</td><td>对输入进行logsoftmax（log of softmax）计算</td><td><strong>【输入】</strong>input：输入Tensor，类型：float32<strong><br><br>【参数】</strong>axis：int，输入变为2D维度时的轴，默认值1<strong><br><br>【约束】</strong>axis参数仅支持最后一维<strong><br><br>【输出】</strong>output：输出Tensor，维度和输入相同</td></tr><tr><td>38</td><td>MatMul</td><td>矩阵乘</td><td><strong>【输入】</strong><br><br>- A：输入Tensor，类型：float32<br>- B：输入Tensor，类型：float32<br><br><strong>【约束】</strong><br><br>- A: 输入tensor，2&lt;=rank&lt;=4<br>- B: 输入tensor，类型与rank同A<br><br><strong>【输出】</strong>Y：输出Tensor，类型：float32</td></tr><tr><td>39</td><td>Max</td><td>逐个元素取最大值</td><td><strong>【输入】</strong><br><br>- X1：输入Tensor，类型：float32<br>- X2：输入Tensor，类型：float32<br><br><strong>【约束】</strong><br><br>- 对于两个输入维度个数不相同的场景，需要将维度补齐到四维。例如，x.shape=(1, 5, 6, 7) 和 y.shape=(6, 7)需要将y的维度补齐到4维，即y.shape=(1, 1, 6, 7)；Kirin 9000平台下，已经支持broadcast；1&lt;=N &lt;= 65535<br>- 只支持2个输入<br><br><strong>【输出】</strong>y：输出Tensor，类型：float32</td></tr><tr><td>40</td><td>MaxPool</td><td>最大池化</td><td><strong>【输入】</strong>X：输入Tensor，类型：float32<strong>【参数】</strong><br><br>- auto_pad：optional, string，pad的计算模式，默认是NOTSET<br>- ceil_mode：optional, int，使用ceil（向上取整）还是floor（向下取整）计算输出维度，默认值0（ceil）<br>- dilations：optional，list of ints，每个值对应池化核对应空间轴上的扩张值，默认值1<br>- kernel_shape：list of ints，每个值对应相应维度的窗口大小<br>- pads：list of ints，每个值对应相应维度的pad值，默认值为0<br>- storage_order：int，Tensor的存储顺序，默认值0，按行存储<br>- strides：list of ints，其中每个值对应相应维度的滑动步长，默认值为1<br><br><strong>【约束】</strong><br><br>- auto_pad参数只支持默认值<br>- storage_order参数只支持默认值<br>- dilations参数只支持默认值<br>- Indices可选输出不支持<br><br><strong>【输出】</strong><br><br>- Y：输出Tensor，类型与X输入相同<br>- Indices：optional，输出Tensor</td></tr><tr><td>41</td><td>Min</td><td>逐个元素取最小</td><td><strong>【输入】</strong><br><br>- X1：输入Tensor，类型：float32, int32<br>- X2：输入Tensor，类型：float32,int32<br><br><strong>【约束】</strong><br><br>- 5D输入不支持常量广播<br>- 只支持2个输入<br><br><strong>【输出】</strong>y：输出Tensor，类型与Xn输入相同</td></tr><tr><td>42</td><td>Mul</td><td>二元点乘</td><td><strong>【输入】</strong><br><br>- A：输入Tensor，类型：float32，第1个操作数<br>- B：输入Tensor，类型：float32，第2个操作数<br><br><strong>【约束】</strong>支持两组输入的维度不一致，进行广播操作（广播即维度补齐），目前支持以下几种广播场景：<br><br>- NCHW+NCHW(备注, 两个维度相同tensor)<br>- NCHW+scalar<br>- NCHW+W, CHW+W, HW+W(备注, W维度做broadcast)<br>- NCHW + NCH1, CHW + CH1, HW + H1<br>- CHW + C1W(备注，H维度做broadcast)<br><br>对于两个输入维度个数不相同的场景，需要将维度补齐到四维。例如，x.shape=(1, 5, 6, 7) 和 y.shape=(6, 7)需要将y的维度补齐到4维，即y.shape=(1, 1, 6, 7)。说明：两个Tensor的输入顺序可以互换。<strong>【输出】</strong>C：输出Tensor，类型同B</td></tr><tr><td>43</td><td>Neg</td><td>对Tensor的每个元素取反，y=-x</td><td><strong>【输入】</strong>X：输入Tensor，类型：float32<strong><br><br>【约束】</strong>无<strong><br><br>【输出】</strong>Y：输出Tensor，类型：float32</td></tr><tr><td>44</td><td>NonMaxSuppression</td><td>进行非最大值压缩在指定框中</td><td><strong>【输入】</strong><br><br>- boxes：输入Tensor，类型：float32<br>- scores：输入Tensor，类型：float32<br>- max_output_boxes_per_class（可选）：输入Tensor，类型：int32<br>- iou_threshold（可选）：输入Tensor，类型：float32<br>- score_threshold（可选）：输入Tensor，类型：float32<br><br><strong>【约束】</strong><br><br>- max_output_boxes_per_class、iou_threshold、score_threshold仅支持权值输入<br>- max_output_boxes_per_class不支持取0<br>- iou_threshold不支持取0和1<br><br><strong>【输出】</strong>selected_indices：输出Tensor，类型：float32</td></tr><tr><td>45</td><td>Or</td><td>对输入Tensor的每个元素取逻辑或</td><td><strong>【输入】</strong><br><br>- A：输入Tensor，类型：bool<br>- B：输入Tensor，类型：bool<br><br><strong>【约束】</strong>input输入不支持NCHW格式数据<strong>【输出】</strong>C：输出Tensor，类型：bool</td></tr><tr><td>46</td><td>PRelu</td><td>根据下面公式做运算f(x) = slope * x for x &lt; 0, f(x) = x for x &gt;= 0</td><td><strong>【输入】</strong><br><br>- X：输入Tensor，类型：float32<br>- slope：输入Tensor，类型：float32<br><br><strong>【约束】</strong><br><br>- slope输入必须是常量<br>- slope必须是标量或者1C11或C11的形式，其中C是X输入的channel<br>- X输入必须是4D<br><br><strong>【输出】</strong>Y：输出Tensor，类型同X输入</td></tr><tr><td>47</td><td>Pad</td><td>对输入Tensor做补pad处理</td><td><strong>【输入】</strong><br><br>- data：输入Tensor，类型：float32<br>- pads：输入Tensor，类型：int64<br>- constant_value：optional，输入Tensor，标量，类型同data输入，默认值0<br><br><strong>【参数】</strong><br><br>- mode：string，支持的模式<br>- 在opset 7~10中pads输入是参数<br>- 在opset 7~10中存在value参数，表示要被填充的值<br><br><strong>【约束】</strong><br><br>- mode仅支持constant模式<br>- pads输入必须是常量<br><br><strong>【输出】</strong>output：输出Tensor，类型：float32</td></tr><tr><td>48</td><td>Pow</td><td>逐个元素做指数运算</td><td><strong>【输入】</strong><br><br>- X：输入Tensor，类型：float32, int32<br>- Y：输入Tensor，类型：float32, int32<br><br><strong>【约束】</strong>无限制<strong>【输出】</strong>Z：输出Tensor，类型：float32, int32</td></tr><tr><td>49</td><td>Range</td><td>创建一个Tensor，Tensor的数据以start开始，以delta作为步长扩展直到limit</td><td><strong>【输入】</strong><br><br>- start：输入Tensor，标量，类型：float32，int32，输出数据的起始值<br>- limit：输入Tensor，标量，类型：float32，int32，输出数据的上限值<br>- delta：输入Tensor，标量，类型：float32，int32，步长<br><br><strong>【约束】</strong><br><br>- 输入start必须是常量<br>- 输入limit必须是常量<br>- 输入delta必须是常量<br><br><strong>【输出】</strong>output：输出Tensor，标量，类型float32，int32</td></tr><tr><td>50</td><td>ReduceLogSumExp</td><td>计算输入Tensor沿着指定轴上的对数和的指数</td><td><strong>【输入】</strong>data：输入Tensor，类型：float32<strong>【参数】</strong><br><br>- axes：list of ints，指定的轴<br>- keepdims：ints，是否保留指定轴上的维度，默认值是1<br><br><strong>【约束】</strong>无<strong>【输出】</strong>reduced：输出Tensor</td></tr><tr><td>51</td><td>ReduceMax</td><td>计算输入Tensor沿着指定轴上的最大值</td><td><strong>【输入】</strong>data：输入Tensor，类型：float32<strong>【参数】</strong><br><br>- axes：list of ints，指定的轴<br>- keepdims：ints，是否保留指定轴上的维度，如果是1，保留，如果是0，则不保留，默认值是1<br><br><strong>【约束】</strong>axes参数为必选参数<strong>【输出】</strong>reduced：输出Tensor</td></tr><tr><td>52</td><td>ReduceMean</td><td>计算输入Tensor沿着指定轴上的平均值</td><td><strong>【输入】</strong>data：输入Tensor，类型：float32<strong>【参数】</strong><br><br>- axes：list of ints，指定的轴<br>- keepdims：ints，是否保留指定轴上的维度，如果是1，保留，如果是0，则不保留，默认值是1<br><br><strong>【约束】</strong>axes参数为必选参数<strong>【输出】</strong>reduced：输出Tensor</td></tr><tr><td>53</td><td>ReduceMin</td><td>计算输入Tensor沿着指定轴上的最小值</td><td><strong>【输入】</strong>data：输入Tensor，类型：float32<strong>【参数】</strong><br><br>- axes：list of ints，指定的轴<br>- keepdims：int，是否保留维度，默认值1<br><br><strong>【约束】</strong><br><br>- axes参数为必选参数<br>- keepdims参数仅支持设置为1<br><br><strong>【输出】</strong>reduced：输出Tensor</td></tr><tr><td>54</td><td>ReduceSum</td><td>计算输入Tensor沿着指定轴上的和</td><td><strong>【输入】</strong>data：输入Tensor，类型：float32<strong>【参数】</strong><br><br>- axes：list of ints，指定的轴<br>- keepdims：int，是否保留维度，默认值1<br><br><strong>【约束】</strong>axes参数为必选参数<strong>【输出】</strong>reduced：输出Tensor</td></tr><tr><td>55</td><td>Relu</td><td>整流线性单位函数</td><td><strong>【输入】</strong>X：输入Tensor，类型：float32<strong><br><br>【约束】</strong>无<strong><br><br>【输出】</strong>Y：输出Tensor，类型同X输入</td></tr><tr><td>56</td><td>Reshape</td><td>改变输入Tensor的维度</td><td><strong>【输入】</strong><br><br>- data：输入Tensor，类型：float32，int32，int64，bool<br>- shape：输入Tensor，类型：int32，int64<br><br><strong>【约束】</strong>shape输入必须是常量<strong>【输出】</strong>reshaped：输出Tensor，类型同data输入</td></tr><tr><td>57</td><td>Resize</td><td>调整输入tensor的维度</td><td><strong>【输入】</strong><br><br>- X：输入Tensor，类型：float32<br>- roi：输入Tensor，类型：float32<br><br><strong>【参数】</strong><br><br>- align_corners：bool，缺省值为false<br>- half_pixel_centers：bool，缺省值为false<br><br><strong>【约束】</strong><br><br>- scales和sizes两个输入仅支持权值输入，不支持非权值输入<br>- coordinate_transformation_mode支持half_pixel，pytorch_half_pixel。其中pytorch_half_pixel仅支持resized_length&gt;1场景，resized_length &lt;=1场景报错<br>- cubic_coeff_a仅支持默认值-0.75<br>- exclude_outside仅支持默认值0<br>- extrapolation_value仅支持默认值：0.0<br>- mode 支持nearest, linear<br>- nearest_mode仅支持默认值round_prefer_floor<br>- 不支持crop功能<br>- roi是无效输入时，仅支持权值输入<br>- 其他场景报错处理<br><br><strong>【输出】</strong>Y：输出Tensor，类型同data输入</td></tr><tr><td>58</td><td>RoiAlign</td><td>对关注的区域做对齐操作</td><td><strong>【输入】</strong><br><br>- X：输入Tensor，类型：float32<br>- roi：输入Tensor，类型：int32<br>- batch_indices：输入Tensor，类型：int32<br><br><strong>【参数】</strong><br><br>- mode：string，缺省值为avg<br>- output_height：int32，缺省值为1<br>- output_width：int32，缺省值为1<br>- sampling_ratio：int32，缺省值为1<br>- spatial_scale：float32，缺省值为1.0<br><br><strong>【约束】</strong>mode仅支持avg模式<strong>【输出】</strong>Y：输出Tensor，类型同X输入</td></tr><tr><td>59</td><td>Round</td><td>逐个元素取整</td><td><strong>【输入】</strong>X：输入Tensor，类型：float32<strong><br><br>【约束】</strong>无限制<strong><br><br>【输出】</strong>Y：输出Tensor，类型同X输入</td></tr><tr><td>60</td><td>ScatterElements</td><td>绘制散点图</td><td><strong>【输入】</strong><br><br>- data：输入Tensor，类型：float32, int8, uint8, bool<br>- indices：输入Tensor，类型：int32, int64<br>- updates：输入Tensor，类型：float32, int8, uint8, bool<br><br><strong>【参数】</strong>axis：int，缺省值为0<strong>【约束】</strong>无<strong>【输出】</strong>output：输出Tensor，类型同data输入</td></tr><tr><td>61</td><td>Selu</td><td>对输入做下面公式计算y = gamma <em>(alpha </em>e^x - alpha) for x &lt;= 0, y = gamma * x for x &gt; 0</td><td><strong>【输入】</strong>x：输入Tensor，类型：float32<strong>【参数】</strong><br><br>- alpha：float32，缺省值为1.67326<br>- gamma：float32，缺省值为1.0507<br><br><strong>【约束】</strong>仅支持alpha和gamma参数是默认值<strong><br><br>【输出】</strong>y：输出Tensor，类型同x输入</td></tr><tr><td>62</td><td>Shape</td><td>获取输入Tensor的维度，并输出</td><td><strong>【输入】</strong>data：输入Tensor，类型：float32，int32，bool，uint8<strong><br><br>【约束】</strong>不支持指定为输出节点<strong><br><br>【输出】</strong>shape：输出Tensor，data输入的维度</td></tr><tr><td>63</td><td>Sigmoid</td><td>sigmoid函数，y = 1 / (1 + exp(-x))</td><td><strong>【输入】</strong>X：输入Tensor，类型：float32<strong><br><br>【约束】</strong>无<strong><br><br>【输出】</strong>Y：输出Tensor，类型同X输入</td></tr><tr><td>64</td><td>Sign</td><td>sign函数（符号函数），当x&gt;0，sign(x)=1当x=0，sign(x)=0当x&lt;0，sign(x)=-1</td><td><strong>【输入】</strong>input：输入Tensor，类型：float32<strong><br><br>【约束】</strong>无<strong><br><br>【输出】</strong>output：输出Tensor，类型和维度同input输入</td></tr><tr><td>65</td><td>Sin</td><td>计算正弦</td><td><strong>【输入】</strong>input：输入Tensor，类型：float32<strong><br><br>【约束】</strong>无限制<strong><br><br>【输出】</strong>output：输出Tensor，类型与x输入相同</td></tr><tr><td>66</td><td>Slice</td><td>对输入Tensor沿着指定轴切分</td><td><strong>【输入】</strong><br><br>- data：输入Tensor，类型：float32，int32，uint8，bool<br>- starts：1-D输入Tensor，类型：int64，int32，表示在指定轴上切分的起始位置<br>- ends：1-D输入Tensor，类型：int64，int32，表示在指定轴上切分的结束位置<br>- axes：optional，1-D输入Tensor，类型：int64，int32，指定需要切分的轴，缺省表示沿所有的维度切分，负值表示从后往前统计维度值<br>- steps：optional，1-D输入Tensor，类型：int64，int32，切分的步长<br><br><strong>【参数】</strong>opset9及之前版本，axes，ends，starts是参数<strong>【约束】</strong><br><br>- 输入starts，ends，axes，steps必须为常量<br>- 不支持切分后，存在维度为0的场景<br><br><strong>【输出】</strong>output：输出Tensor，切分后的Tensor</td></tr><tr><td>67</td><td>Softmax</td><td>归一化逻辑函数</td><td><strong>【输入】</strong>input：输入Tensor，类型：float32<strong><br><br>【参数】</strong>axis：int，输入变为2D维度时的轴，默认值1<strong><br><br>【约束】</strong>axis参数仅支持最后一维<strong><br><br>【输出】</strong>output：输出Tensor，类型和维度同input输入</td></tr><tr><td>68</td><td>Softplus</td><td>softplus激活函数，y = ln(exp(x) + 1)</td><td><strong>【输入】</strong>X：1-D输入Tensor，类型：float32<strong><br><br>【约束】</strong>无<strong><br><br>【输出】</strong>Y：1-D输出Tensor，类型同X输入</td></tr><tr><td>69</td><td>Split</td><td>在指定轴上做拆分,输出多个tensor</td><td><strong>【输入】</strong>input：输入Tensor，类型： float32<strong>【参数】</strong><br><br>- axis：int, 缺省值为0<br>- split: list of ints 值必须大于等于0<br><br><strong>【约束】</strong>无<strong><br><br>【输出】</strong>outputs：D输出Tensor，类型同输入</td></tr><tr><td>70</td><td>Sqrt</td><td>求平方根</td><td><strong>【输入】</strong>X：输入Tensor，类型：float32<strong><br><br>【约束】</strong>无<strong><br><br>【输出】</strong>Y：输出Tensor，类型同X输入</td></tr><tr><td>71</td><td>Squeeze</td><td>在指定轴上降维</td><td><strong>【输入】</strong>data：输入Tensor，类型：float32<strong><br><br>【参数】</strong>axes：list of ints，轴，缺省值为所有维度，负值表示从后往前遍历<strong><br><br>【约束】</strong>无<strong><br><br>【输出】</strong>squeezed：输出Tensor，类型：float32</td></tr><tr><td>72</td><td>Sub</td><td>逐个元素的减法运算</td><td><strong>【输入】</strong><br><br>- A：输入Tensor，类型：float32 ，第1个操作数<br>- B：输入Tensor，类型：float32 ，第2个操作数<br><br><strong>【约束】</strong>无<strong><br><br>【输出】</strong>C：输出Tensor，类型：float32</td></tr><tr><td>73</td><td>Tan</td><td>正切函数</td><td><strong>【输入】</strong>input：输入Tensor，类型：float32<strong><br><br>【约束】</strong>无<strong><br><br>【输出】</strong>output：输出Tensor，类型：float32</td></tr><tr><td>74</td><td>Tanh</td><td>双曲函数</td><td><strong>【输入】</strong>input：输入Tensor，类型：float32<strong>【约束】</strong>无<strong>【输出】</strong>output：输出Tensor，类型：float32</td></tr><tr><td>75</td><td>Tile</td><td>对输入Tensor做平铺操作</td><td><strong>【输入】</strong><br><br>- input：输入Tensor，类型：float32，int8，uint8，bool<br>- repeats：1-D输入Tensor，类型：int32, int64<br><br><strong>【约束】</strong>repeats输入必须是常量<strong>【输出】</strong>output：输出Tensor，类型同input输入</td></tr><tr><td>76</td><td>TopK</td><td>实现最大或者最小的K个元素在指定的轴上。</td><td><strong>【输入】</strong><br><br>- X：输入Tensor，类型：float32<br>- K：输入Tensor，类型：int32, int64<br><br><strong>【参数】</strong><br><br>- axis：int，缺省值为-1<br>- largest：int，缺省值为1<br>- sorted：int，缺省值为1<br><br><strong>【约束】</strong>K输入必须是常量<strong>【输出】</strong><br><br>- values：输出Tensor，类型：float32<br>- indices: 输出Tensor，类型：int32</td></tr><tr><td>77</td><td>Transpose</td><td>根据属性perm中各个轴的排列顺序，对输入Tensor和shape做相应转换</td><td><strong>【输入】</strong>data：输入Tensor，类型：float32<strong><br><br>【参数】</strong>perm：list of ints，轴调整排列的顺序表<strong><br><br>【约束】</strong>无<strong>【输出】</strong>transposed：输出Tensor，类型：float32</td></tr><tr><td>78</td><td>Unsqueeze</td><td>在指定轴上扩维</td><td><strong>【输入】</strong>data：输入Tensor，类型：float32，int32，uint8，bool<strong><br><br>【参数</strong>】axes：list of ints，指定需要扩维的轴<br><br><strong>【约束】仅支持axes个数为1</strong><br><br><strong>【输出】expanded：输出Tensor，类型：float32，int32，uint8，bool</strong></td></tr><tr><td>79</td><td>Upsample</td><td>上采样</td><td><strong>【输入】</strong><br><br>- X：输入Tensor，类型：float32<br>- scales：输入Tensor，类型：float32<br><br><strong>【参数】</strong><br><br>- mode：string，模式，有2个插值模式，nearest模式和linear模式（包含bilinear, trilinear等），默认是nearest模式<br>- 在opset7中scales输入是参数<br><br><strong>【约束】</strong><br><br>- 仅支持nearest、bilinear两种插值模式<br>- X仅支持4D输入Tensor<br>- N、C方向的scale仅支持等于1.0<br><br><strong>【输出】</strong>Y：输出Tensor，类型：float32</td></tr><tr><td>80</td><td>ReduceL2（V510新增）</td><td>计算输入Tensor沿着指定轴上的欧几里得范数</td><td><strong>【输入】</strong>data：输入Tensor，类型：float32<strong>【参数】</strong><br><br>- axes：list of ints，指定的轴<br>- keepdims：int，是否保留维度，默认值1<br><br><strong>【约束】</strong><br><br>- 只支持keepdims=1的情形，所以输入和输出的realDim相等<br>- 输入N &lt;= 65535<br>- axes范围：支持realDim为3和realDim为4时对最后一维做reduceL2<br><br><strong>【输出】</strong>reduced：输出Tensor</td></tr></tbody></table></div></div><div class="article-licensing box"><div class="licensing-title"><p>ONNX算子简介</p><p><a href="https://cyhasuka.github.io/posts/5ae3bedd/">https://cyhasuka.github.io/posts/5ae3bedd/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>cyhasuka</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2024-04-01</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2024-04-01</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Linux/">Linux</a><a class="link-muted mr-2" rel="tag" href="/tags/%E6%84%9F%E7%9F%A5%E7%AE%97%E6%B3%95/">感知算法</a><a class="link-muted mr-2" rel="tag" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div></article></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/posts/186ec257/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Nvidia GPU &amp; 算能Sophon TPU 算子耗时分析</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/posts/764ef83/"><span class="level-item">3D-NMS 算法及PCL &amp; CUDA实现</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div class="content twikoo" id="twikoo"></div><script src="/js/twikoo.all.min.js"></script><script>twikoo.init({envId:"https://cyhasuka.netlify.app/.netlify/functions/twikoo",lang:"zh-CN"})</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/profilewbp.webp" alt="Yuhao Cui"></figure><p class="title is-size-4 is-block" style="line-height:inherit">Yuhao Cui</p><p class="is-size-6 is-block">Lidar Perception Algorithm &amp; Robotics</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Suzhou,China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">14</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">17</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/cyhasuka" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/cyhasuka"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="ZhiHu" href="https://www.zhihu.com/people/cui-yu-hao-77"><i class="fab fa-zhihu"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Mail" href="mailto:mail@cuiyuhao.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="GMail" href="mailto:cyhasuka@gmail.com"><i class="fab fa-google"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/%E5%86%99%E7%82%B9%E6%95%99%E7%A8%8B/"><span class="level-start"><span class="level-item">写点教程</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%86%99%E7%82%B9%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">写点笔记</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%9D%A5%E7%82%B9%E5%B9%B2%E8%B4%A7/"><span class="level-start"><span class="level-item">来点干货</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%8E%A9%E7%89%A9%E6%9C%89%E5%BF%97/"><span class="level-start"><span class="level-item">玩物有志</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time datetime="2024-04-12T02:30:05.000Z">2024-04-12</time></p><p class="title"><a href="/posts/186ec257/">Nvidia GPU &amp; 算能Sophon TPU 算子耗时分析</a></p><p class="categories"><a href="/categories/%E5%86%99%E7%82%B9%E6%95%99%E7%A8%8B/">写点教程</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2024-04-01T09:04:05.000Z">2024-04-01</time></p><p class="title"><a href="/posts/5ae3bedd/">ONNX算子简介</a></p><p class="categories"><a href="/categories/%E6%9D%A5%E7%82%B9%E5%B9%B2%E8%B4%A7/">来点干货</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2024-03-04T10:34:20.000Z">2024-03-04</time></p><p class="title"><a href="/posts/764ef83/">3D-NMS 算法及PCL &amp; CUDA实现</a></p><p class="categories"><a href="/categories/%E5%86%99%E7%82%B9%E7%AC%94%E8%AE%B0/">写点笔记</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2024-03-01T09:04:05.000Z">2024-03-01</time></p><p class="title"><a href="/posts/26007bd4/">FastPillars-论文笔记-激光雷达点云感知算法</a></p><p class="categories"><a href="/categories/%E5%86%99%E7%82%B9%E7%AC%94%E8%AE%B0/">写点笔记</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2024-01-24T02:39:05.000Z">2024-01-24</time></p><p class="title"><a href="/posts/8a630bae/">NVIDIA GPU 架构与 CUDA 算力</a></p><p class="categories"><a href="/categories/%E6%9D%A5%E7%82%B9%E5%B9%B2%E8%B4%A7/">来点干货</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">文章</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/"><span class="level-start"><span class="level-item">2024</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/"><span class="level-start"><span class="level-item">2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/"><span class="level-start"><span class="level-item">2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Autoware/"><span class="tag">Autoware</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hexo/"><span class="tag">Hexo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NVIDIA/"><span class="tag">NVIDIA</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ROG/"><span class="tag">ROG</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ROS/"><span class="tag">ROS</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Sophon%E7%AE%97%E8%83%BD/"><span class="tag">Sophon算能</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A4%96%E8%AE%BE/"><span class="tag">外设</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%98%E5%82%A8%E8%AE%BE%E5%A4%87/"><span class="tag">存储设备</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%84%9F%E7%9F%A5%E7%AE%97%E6%B3%95/"><span class="tag">感知算法</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%91%84%E5%83%8F%E5%A4%B4/"><span class="tag">摄像头</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="tag">深度学习</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE/"><span class="tag">激光雷达</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85/"><span class="tag">系统安装</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%B3%BB%E7%BB%9F%E9%97%AE%E9%A2%98/"><span class="tag">系统问题</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87/"><span class="tag">网络设备</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/"><span class="tag">自动驾驶</span><span class="tag">5</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/toplogo.jpg" alt="CuiYuhao&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2024 cyhasuka</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客,<span id="busuanzi_value_site_pv">0</span>次访问</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="知识共享" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="CC BY 4.0" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="我的GitHub" href="https://github.com/cyhasuka"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn")</script><script>var IcarusThemeSettings={article:{highlight:{clipboard:!0,fold:"unfolded"}}}</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load",()=>{window.cookieconsent.initialise({type:"info",theme:"edgeless",static:!1,position:"bottom-left",content:{message:"我们会使用Cookies来改善您的体验。",dismiss:"知道了！",allow:"允许使用Cookies",deny:"拒绝",link:"了解更多",policy:"Cookies政策",href:"https://www.cookiesandyou.com/"},palette:{popup:{background:"#edeff5",text:"#838391"},button:{background:"#4b81e8"}}})})</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load",()=>{"function"==typeof $.fn.lightGallery&&$(".article").lightGallery({selector:".gallery-item"}),"function"==typeof $.fn.justifiedGallery&&($(".justified-gallery > p > .gallery-item").length&&$(".justified-gallery > p > .gallery-item").unwrap(),$(".justified-gallery").justifiedGallery())})</script><script src="/js/main.js" defer></script><script src="/js/chuckle-post-ai.js"></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener("DOMContentLoaded",(function(){loadInsight({contentUrl:"/content.json"},{hint:"想要查找什么...",untitled:"(无标题)",posts:"文章",pages:"页面",categories:"分类",tags:"标签"})}))</script></body></html>